{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1e72bbe8-b741-4bef-a4df-2c6a6c705d23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1623289980.py, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[92], line 52\u001b[1;36m\u001b[0m\n\u001b[1;33m    if\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import vocabulary\n",
    "import ngram\n",
    "\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s %(asctime)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "def load_config():\n",
    "    \"\"\"\n",
    "    Fonction permettant de charger le fichier de configuration config.json et de le charger en tant que dictionnaire Python.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Loading configuration...\")\n",
    "        with open('config.json') as json_config_file:\n",
    "            config = json.load(json_config_file)\n",
    "        logging.info(\"Loaded.\")\n",
    "        return config\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error while loading configuration file 'config.json'.\")\n",
    "        logging.error(e)\n",
    "        logging.error(\"Exiting.\")\n",
    "        sys.exit(1)\n",
    "        raise\n",
    "\n",
    "def load_models(generate_name):\n",
    "    logging.info(\"Loading models...\")\n",
    "    sub_folders = glob(f\"data/ngram/{generate_name}/*\")\n",
    "    models = {}\n",
    "    for folder in sub_folders:\n",
    "        n = int(folder.split(\"\\\\\")[-1])\n",
    "        logging.info(f\"\\tLoading Ngram model, n={n}...\")\n",
    "\n",
    "        temp_ngram = ngram.Ngram()\n",
    "        path_model = glob(f\"{folder}/*.ngram\")\n",
    "        temp_ngram.load(path_model[0])\n",
    "\n",
    "        models[n] = temp_ngram\n",
    "\n",
    "    return models\n",
    "\n",
    "def generate_sentence(models, vocab, starts_with, min_words, end_char):\n",
    "    sentence = starts_with\n",
    "    word_count = 1\n",
    "    model = models[1]\n",
    "    \n",
    "    while word_count <= min_words:\n",
    "        # Tokenize the sentence\n",
    "        sentence_tokenized = vocab.chain_to_ids(sentence).split()\n",
    "        \n",
    "        # Select probas associated to the actual word/sequence\n",
    "        probas = model.chain_frequency.get(int(sentence_tokenized[-1]))\n",
    "        \n",
    "        # Select the next word \n",
    "        next_word = random.choices(list(probas.keys()), weights=list(probas.values()), k=1)[0]\n",
    "        \n",
    "        # Add the new word to the sentence\n",
    "        sentence_tokenized.append(str(next_word))\n",
    "        \n",
    "        # Un-Tokenize\n",
    "        sentence = vocab.ids_to_chain(\" \".join(sentence_tokenized))\n",
    "        word_count = len(sentence.split())\n",
    "        \n",
    "        # Is last char in end_char\n",
    "        if word_count > min_words and sentence[-1] not in end_char:\n",
    "            min_words += 1\n",
    "        \n",
    "        \n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9fa4b659-e913-4c44-b14c-f6073263d5bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2023-04-19 00:23:17,806 - Generating 10 sentences...\n",
      "INFO 2023-04-19 00:23:18,069 - Le Pen : « Il faut d'abord celle des débouchés évidentes.\n",
      "INFO 2023-04-19 00:23:18,527 - Le Pen… EMMANUEL MACRON Complètement. Résolument. Dordogne, il y a donc vous féliciter.\n",
      "INFO 2023-04-19 00:23:18,687 - Le deuxième chose, on a néanmoins un travail aussi n'y aura des ports français.\n",
      "INFO 2023-04-19 00:23:18,722 - Le jour à limiter les entreprises. Nous partageons le nouveau système commercial qui la recherche fondamentale et nous faudra la République, et technologique  il y ait une réunion du Parlement qui seront jamais perdu avec vous avez envoyé à ce qui favorise la Culture.\n",
      "INFO 2023-04-19 00:23:18,953 - Le mot est de la semaine où c'était le ministre des choix familiaux qui vivent dans le faire évoluer les réformes que nous disent les travailleurs détachés parce que nous mériterons leur travail est à soutenir l'Ukraine après un cadre législatif au corps à cette route que c'était à la lumière un message d'unité et à la génération libérée ; la base d'un conflit dans notre rôle, c'est aussi une offre numérique de jugement à la Maison Blanche, en permanence, l'un des événements, nous y a été avocat, parce que nous devons sceller un espace neutre, je laisse Marseille se feront.\n",
      "INFO 2023-04-19 00:23:19,345 - Le premier, c'est aussi urgent, peut-être pour avoir comme les clés.\n",
      "INFO 2023-04-19 00:23:19,504 - Le troisième, c'est aussi ou la fin des mensonges. C'est l'objet d'investissements ou dans l'intérêt de l'intermittence des accords obtenus afin qu'elle est fait qu'il va.\n",
      "INFO 2023-04-19 00:23:19,938 - Le Secrétaire général, n'a pas de fait, ils sont au fond du monde qui grève doit s'en souvient, on doit prendre, beaucoup de sécurité soient législatives, un chemin, une première, et travaillent, artisans, nos pôles, ça ne vous avez commencé pour que les Lumières, qui vont avec.\n",
      "INFO 2023-04-19 00:23:20,726 - Le Pen : production non respect de la Bulgarie sur toute honnêteté  et contrôler les commentaires où, vous puissiez nous mobilisions de la formation de cette université que nous devons continuer à chaque jour.\n",
      "INFO 2023-04-19 00:23:21,251 - Le Pen « on ne sont ainsi la lumière de la même d'avoir un parc d'attraction, un objectif à la réconciliation et de flexibilité donnée à affronter en France, Monsieur le continent.\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"Generating {nb_sentences_to_generate} sentences...\")\n",
    "sentences = [generate_sentence(models, vocab, starts_with, min_words, end_char) for _ in range(nb_sentences_to_generate)]\n",
    "\n",
    "for sentence in sentences:\n",
    "    time.sleep(random.uniform(0, delay+1))\n",
    "    logging.info(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9ab02035-1369-4b26-ae6c-6772dfc39c35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2023-04-19 00:20:18,743 - Loading configuration...\n",
      "INFO 2023-04-19 00:20:18,745 - Loaded.\n"
     ]
    }
   ],
   "source": [
    "config = load_config()\n",
    "generate_name = config[\"generate_name\"]\n",
    "nb_sentences_to_generate = config[\"nb_sentences_to_generate\"]\n",
    "starts_with = config[\"starts_with\"]\n",
    "delay = config[\"delay\"]\n",
    "starts_with = config[\"starts_with\"]\n",
    "min_words = config[\"min_words\"]\n",
    "end_char = config[\"end_char\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce11d06a-f9b0-4db8-b8cd-ab7814f7391c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2023-04-19 00:09:44,810 - Loading configuration...\n",
      "INFO 2023-04-19 00:09:44,829 - Loaded.\n",
      "INFO 2023-04-19 00:09:44,829 - Loading vocabulary...\n",
      "INFO 2023-04-19 00:09:44,891 - Loaded.\n",
      "INFO 2023-04-19 00:09:44,891 - Loading models...\n",
      "INFO 2023-04-19 00:09:44,891 - \tLoading Ngram model, n=1...\n",
      "INFO 2023-04-19 00:09:45,013 - \tLoading Ngram model, n=2...\n",
      "INFO 2023-04-19 00:09:45,616 - \tLoading Ngram model, n=3...\n",
      "INFO 2023-04-19 00:09:46,874 - \tLoading Ngram model, n=4...\n",
      "INFO 2023-04-19 00:09:48,718 - \tLoading Ngram model, n=5...\n",
      "INFO 2023-04-19 00:09:50,796 - 5 models loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: <ngram.ngram.Ngram object at 0x000001FAFC32D290>, 2: <ngram.ngram.Ngram object at 0x000001FAFD0468D0>, 3: <ngram.ngram.Ngram object at 0x000001FAFB8610D0>, 4: <ngram.ngram.Ngram object at 0x000001FA99F24E90>, 5: <ngram.ngram.Ngram object at 0x000001FA99F25450>}\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"Loading vocabulary...\")\n",
    "vocab = vocabulary.Vocabulary()\n",
    "vocab.load(f\"data/vocabs/{generate_name}.vocab\")\n",
    "logging.info(f\"Loaded.\")\n",
    "\n",
    "models = load_models(generate_name)\n",
    "logging.info(f\"{len(models)} models loaded.\")\n",
    "\n",
    "print(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
