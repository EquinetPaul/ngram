{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NGRAM for NLP","text":""},{"location":"#what-is-it","title":"What is it?","text":"<p>NGRAM for NLP is a set of scripts and algorithms for training and using probabilistic/statistical  generative language models such as n-grams</p> <p>Since the emergence of Transformer models in 2017 with the scientific paper \"Attention Is All You Need\", which led to the appearance of highly performant generative language models demonstrating capabilities never seen before, this project was initiated based on a state-of-the-art of generative language models proposed by Paul Equinet as part of his final year project to complete his engineering degree in applied mathematics at CyTech Pau.</p> <p>The aim of this project and its associated documentation is to provide pedagogical tools for understanding language models.</p>"},{"location":"#generative-language-models","title":"Generative Language Models","text":"<p>Generative language models are artificial intelligence models that learn to produce text, speech, or other forms of linguistic data similar to those produced by humans.</p> <p>If I give you the phrase</p> <ul> <li>The capital of France is...</li> </ul> <p>you would be tempted to answer 'Paris'.</p> <p>For the phrase</p> <ul> <li>The cat eats the...</li> </ul> <p>you would probably answer 'mouse'.</p> <p>Well, that's what language models do. They use the data given to them and select/predict THE next word that has the highest probability of appearing next.</p> <p>In an iterative process, it uses the input sequence given to it and the sequences it has generated to produce the next sequence. In this way, it constructs entire sentences or entire paragraphs.</p> <p>Dans la phrase \"The capital of France is...\", ce qui nous pousse \u00e0 r\u00e9pondre \"Paris\" est le fait que l'on a capt\u00e9 les mots \"capital\" et \"France\" et qu'en les associant, le mot qui nous vient ensuite est \"Paris\". En quelque sorte, notre esprit a fait la somme de ces deux termes et en a conclu qu'il en r\u00e9sultait \"Paris\". On pourrait r\u00e9sumer ce m\u00e9canisme \u00e0:</p> <p>\"capital\" + \"France\" = \"Paris\"</p> <p>Ce mecanisme qui est naturel chez nous est ce que l'on appelle un mechanisme d'attention. Notre cerveau a associ\u00e9 diff\u00e9rentes informations via ce mechanisme pour produire quelque chose.</p> <p>Maintenant, si l'on souhaite entra\u00eener un mod\u00e8le \u00e9tant capable de reproduire de mechanisme d'attention, il faudrait se pencher sur les algorithmes de r\u00e9seaux de neurones profonds utilisant une structure Transformer ou r\u00e9currente.</p> <p>Cependant, \u00e9tant donn\u00e9 que ce projet a pour but de nous initier aux mod\u00e8les de langage g\u00e9n\u00e9ratifs, nous allons r\u00e9fl\u00e9chir \u00e0 un moyen plus simple, en terme de compr\u00e9hension, pour entra\u00eener et utiliser ce genre de mod\u00e8le.</p> <p>Si je souhaite entra\u00eener un mod\u00e8le capable de pr\u00e9dire le mot qui suit une s\u00e9quence donn\u00e9e et que je vous donne la phrase:</p> <ul> <li>the present and the future</li> </ul> <p>L'une des m\u00e9thode qui nous vient en t\u00eate est de se dire: je prends les mots un \u00e0 un et je regarde le mot qui vient apr\u00e8s.</p> <p>De cette mani\u00e8re j'obtiens la fr\u00e9quence d'apparition d'un mot par rapport \u00e0 un autre et donc la probabilit\u00e9 par exemple que le mot \"the\" soit suivi de \"present\" ou de \"future\".</p> <p>C'est de cette mani\u00e8re que l'on construire des mod\u00e8les de langage probabilistes ou fr\u00e9quentiels que l'on appelle aussi des N-gram.</p>"},{"location":"#probabilistic-models","title":"Probabilistic Models","text":"<p>Comme vu pr\u00e9cedemment, nous pouvons \u00e9tudier des phrases en prenant chacun des mots qui la composent et d\u00e9terminer la probabilit\u00e9 qu'un mot puisse succ\u00e8der \u00e0 un autre.</p> <p>La m\u00e9thode utilis\u00e9e permet d'obtenir des probabilit\u00e9s pour des mots les uns apr\u00e8s les autres.</p> <p>C'est \u00e0 dire que pour la phrase \"the present and the\" on ne se basera que sur le mot the pour pr\u00e9dire le prochain mot. Dans ce cas, on aurait:</p> <ul> <li>50% de chance de pr\u00e9dire future</li> <li>50% de chance de pr\u00e9dire present</li> </ul> <p>Dans le GIF pr\u00e9cedent on voit comment on peut calculer les probabilit\u00e9s sur une phrase (the present and the future) mais l'objectif serait d'avoir un corpus de phrase de grande taille pour varier les combinaisons de phrases et donc d'associations de mots. Ce qui nous permettrait de varier les g\u00e9n\u00e9rations et de limiter les r\u00e9p\u00e9titions.</p> <p>Malgr\u00e9 cela, le fait de ne se contenter que du mot pr\u00e9c\u00e9dent pour pr\u00e9dire celui d'apr\u00e8s peut mener \u00e0 certaines g\u00e9n\u00e9rations non pertinentes voire absurdes.</p> <p>Par exemple, si on entra\u00eene un mod\u00e8le sur les phrases:</p> <ul> <li>the chiken is in the farm</li> <li>the shark is in the sea</li> </ul> <p>Il sera tr\u00e8s probable si en entr\u00e9e on lui donne the chicken is in the qu'il compl\u00e8te par sea.</p> <ul> <li>the chiken is in the sea</li> </ul> <p>Surprenant! Mais bon c'est logique \u00e9tant donn\u00e9 que le mod\u00e8le s'est entra\u00een\u00e9 \u00e0 pr\u00e9dire la suite d'une phrase en ne se basant que sur le dernier mot.</p> <p>Pour rem\u00e9dier \u00e0 ce probl\u00e8me, on pourrait envisager de prendre en consid\u00e9ration plus d'\u00e9l\u00e9ments dans la phrase, plus de mots pour avoir plus de contexte.</p> <p>C'est \u00e0 ce moment qu'interviennent les N-grams!</p>"},{"location":"#n-gram","title":"N-gram","text":"<p>En traitement du langage naturel (NLP), les N-grams font partie des mod\u00e8les de langage g\u00e9n\u00e9ratifs probabilistes. Ils utilisent les s\u00e9quences des \"n\" mots pr\u00e9c\u00e9dents pour s'entra\u00eener et fournir la probabilit\u00e9 du mot suivant.</p> <p>Dans les exemples pr\u00e9c\u00e9dents, nous prenions uniquement le mot pr\u00e9c\u00e9dent pour pr\u00e9dire celui d'apr\u00e8s, nous \u00e9tions dans le cas d'un 1-gram ou unigramme.</p> <p>Voici la liste des diff\u00e9rents types de n-grams et exemples de g\u00e9n\u00e9ration</p> <p>input context generated word associated probability</p> <ul> <li> <p>n=1, unigramme</p> <ul> <li>the shark is in the chicken - 0.25</li> <li>the shark is in the shark - 0.25</li> </ul> </li> <li> <p>n=2, bigramme</p> <ul> <li>the shark is in the farm - 0.5</li> </ul> </li> <li> <p>n=3, trigramme</p> <ul> <li>the shark is in the farm - 0.5</li> </ul> </li> <li> <p>n=4, quadrigramme</p> <ul> <li>the shark is in the sea - 1.0</li> </ul> </li> <li> <p>...</p> </li> </ul> <p>On pourrait construire des n-grams tr\u00e8s grands pour conserver le plus de contexte possible et obtenir des pr\u00e9dictions justes mais il faut se rappeler que les n-grams sont des mod\u00e8les probabilistes. C'est \u00e0 dire qu'ils fournissent des probabilit\u00e9s uniquement sur les donn\u00e9es sur lesquelles ils ont \u00e9t\u00e9 entrain\u00e9s. De ce fait, plus on agrandira la fen\u00eatre de contexte, plus le mod\u00e8le apprendra effectivement des relations pr\u00e9cises mais ces relations seront fortement influenc\u00e9es par la taille du corpus d'entra\u00eenement.</p> <p>Si dans mon corpus j'ai des phrases courtes, disons de longueure fixe maximale 6, et que j'entra\u00eene un mod\u00e8le n-gram o\u00f9 n=5, le mod\u00e8le sera en mesure de reproduire exactement les phrases du corpus \u00e9tant donn\u00e9 que sa fen\u00eatre de contexte couvrira presque la phrase enti\u00e8re.</p> <p>Mais \u00e0 partir du moment o\u00f9 on agrandit le corpus, on augmente les combinaisons possibles et de cette mani\u00e8re on introduit plus de diversit\u00e9 dans les probabilit\u00e9s et donc dans les possiblit\u00e9s de g\u00e9n\u00e9ration.</p> <p>Comme vu plus haut pour les unigrammes, les n-grams prendront des s\u00e9quences des n pr\u00e9d\u00e9cents mot pour pr\u00e9dire le prochains sur toute la phrase.</p> <p>Exemple avec n=3:</p> <p>n sequence next</p> <ul> <li>the chiken is in the farm</li> <li>the chiken is in the farm</li> <li>the chiken is in the farm</li> </ul> <p>Nous aurons donc un mod\u00e8le n-gram o\u00f9 n=3, qui sur cette phrase, ressemblerait \u00e0:</p> <pre><code>{\n  \"the chiken is\" : {\"in\" : 1},\n  \"chiken is in\" : {\"the\" : 1},\n  \"is in the\" : {\"farm\" : 1}\n}\n</code></pre> <p>Ce projet a pour but de mettre en place un algorithme permettant d'entra\u00eener et d'utiliser des n-grams.</p>"},{"location":"#ressources","title":"Ressources","text":"<p>More ressources for more understanding</p> <ul> <li>N-gram Wikipedia</li> <li>Understanding Word N-grams and N-gram Probability in Natural Language Processing</li> </ul>"},{"location":"config/","title":"NGRAM for NLP","text":""},{"location":"config/#configuration","title":"Configuration","text":"<p>Before starting the N-gram models training, you need to modify config.json.</p> <p>This file includes all the parameters necessary to perform the training and use of the models.</p>"},{"location":"config/#default-parameters","title":"Default parameters","text":"<ul> <li>generate_name : project name</li> <li> <p>source : Path of the data file(s)</p> </li> <li> <p>data_type : type of data</p> </li> </ul> Values Description df Load dataframe files Load files in the folder <ul> <li> <p>file_extension (optional): extension that files must have (if select, depending on data_type)</p> </li> <li> <p>source_params (optional, if data_type==\"df\")</p> <ul> <li>df_column : column to select data from</li> <li>df_parameters : DataFrame opening parameters (used in pandas.read_csv())</li> </ul> </li> <li> <p>parallelize : True/False if we want the training to be parallelized</p> </li> <li> <p>dask_distributed : True/False if we want the training to be made on a Dask Cluster</p> </li> </ul>"},{"location":"config/#training-parameters","title":"Training parameters","text":"<ul> <li>ngram_range_min</li> <li>ngram_range_max</li> </ul> <p>N-gram(s) will be generated in the range [ngram_range_min;ngram_range_max]</p>"},{"location":"config/#generator-parameters","title":"Generator parameters","text":"<ul> <li>nb_sentences_to_generate : number of sentence(s) to generate</li> <li>delay : delay of appearance of generated sentences</li> <li>starts_with : string by which generated sentences should start</li> <li>min_words : minimum number of word needed in the generated sentence</li> <li>end_char : list of characters by which a generated sentence should end</li> <li>display_model_used: True/False if we want to show the model used at each word generation</li> <li>max_ngram_to_use: sets the value that will limit the model to be used for generating sentences</li> </ul>"},{"location":"config/#dask-parameters","title":"Dask Parameters","text":"<ul> <li>scheduler_address : IP of the Scheduler.</li> <li>local_scheduler_address : local IP:PORT of the Scheduler.</li> </ul>"},{"location":"installation/","title":"NGRAM for NLP","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>Developped with Python v3.11.3.</p>"},{"location":"installation/#clone-the-project","title":"Clone the project","text":"<pre><code>git clone https://github.com/EquinetPaul/ngram_nlp.git\n</code></pre>"},{"location":"installation/#install-libraries","title":"Install libraries","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"usage/","title":"NGRAM for NLP","text":""},{"location":"usage/#commands","title":"Commands","text":"<p>Bienvenue</p>"}]}